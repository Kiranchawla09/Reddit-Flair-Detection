{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALLING MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\h\\anaconda3\\lib\\site-packages (6.3.1)\n",
      "Requirement already satisfied: update-checker>=0.16 in c:\\users\\h\\anaconda3\\lib\\site-packages (from praw) (0.16)\n",
      "Requirement already satisfied: prawcore<2.0,>=1.0.1 in c:\\users\\h\\anaconda3\\lib\\site-packages (from praw) (1.0.1)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\h\\anaconda3\\lib\\site-packages (from praw) (0.56.0)\n",
      "Requirement already satisfied: requests>=2.3.0 in c:\\users\\h\\anaconda3\\lib\\site-packages (from update-checker>=0.16->praw) (2.21.0)\n",
      "Requirement already satisfied: six in c:\\users\\h\\anaconda3\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\h\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\h\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2018.11.29)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in c:\\users\\h\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\h\\anaconda3\\lib\\site-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\h\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "!pip install praw\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import praw as pr                   #module for reddit\n",
    "from bs4 import BeautifulSoup       #Access the HTML of the webpage and extract useful information/data from it.\n",
    "                                    #It is of ‘string’ type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "reddit = pr.Reddit(client_id='#', \n",
    "                     client_secret='#',\n",
    "                     user_agent='#',\n",
    "                     username='#', \n",
    "                     password='#')\n",
    "\n",
    "print(reddit.read_only)  \n",
    "reddit.read_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## India SubReddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 5 organisations of the topics in the subreddit\n",
    "#that is, hot, new, controversial, top, rising\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"Policy/Economy\",\"AMA\", \"Politics\",\"[R]eddiquette\", \n",
    "          \"Photography\",\"Business/Finance\",\"Science/Technology\",\"Sports\", \"Scheduled\",\"Food\"]\n",
    "\n",
    "my_dict = { \"Flair\":[],\n",
    "             \"Title\":[],\n",
    "             \"Created_at\":[],\n",
    "             \"Score\":[],\n",
    "             \"Url_address\":[],\n",
    "             \"Comments\":[],\n",
    "             \"#Comments\":[],\n",
    "             \"Body\":[],\n",
    "             \"Author\":[],\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in flairs:\n",
    "    collect_subreddit= subreddit.search (f,limit=1000)\n",
    "    #collecting 1000 data for each flair \n",
    "    for i in collect_subreddit:\n",
    "        \n",
    "        my_dict[\"Flair\"].append (f)\n",
    "        my_dict[\"Title\"].append (i.title)\n",
    "        my_dict[\"Created_at\"].append (i.created)\n",
    "        my_dict[\"Score\"].append (i.score)\n",
    "        my_dict[\"Url_address\"].append(i.url)\n",
    "        my_dict[\"#Comments\"].append (i.num_comments)\n",
    "        my_dict[\"Body\"].append(i.selftext)\n",
    "        my_dict[\"Author\"].append (i.author)\n",
    "\n",
    "        i.comments.replace_more(limit=None)\n",
    "        comment = ''\n",
    "        for top_level_comment in i.comments:\n",
    "               comment = comment + ' ' + top_level_comment.body\n",
    "        my_dict[\"Comments\"].append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "arrays must all be same length",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-0b41b4f3f68c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmy_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    346\u001b[0m                                  dtype=dtype, copy=copy)\n\u001b[0;32m    347\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mmgr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaskedArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmrecords\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmrecords\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_init_dict\u001b[1;34m(self, data, index, columns, dtype)\u001b[0m\n\u001b[0;32m    457\u001b[0m             \u001b[0marrays\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_arrays_to_mgr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_init_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arrays_to_mgr\u001b[1;34m(arrays, arr_names, index, columns, dtype)\u001b[0m\n\u001b[0;32m   7354\u001b[0m     \u001b[1;31m# figure out the index, if necessary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7355\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7356\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7357\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7358\u001b[0m     \u001b[1;31m# don't force copy because getting jammed in an ndarray anyway\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mextract_index\u001b[1;34m(data)\u001b[0m\n\u001b[0;32m   7400\u001b[0m             \u001b[0mlengths\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_lengths\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7401\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7402\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'arrays must all be same length'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7404\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhave_dicts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: arrays must all be same length"
     ]
    }
   ],
   "source": [
    "my_data = pd.DataFrame(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "def word_count (text):\n",
    "    return \n",
    "    \n",
    "words = my_data[\"Body\"].apply(lambda x: len(str(x).split(\" \")))   \n",
    "my_data= my_data.assign (Word_Count= words)      #no. of words in the body\n",
    "\n",
    "\n",
    "chars= my_data[\"Body\"].str.len()\n",
    "my_data= my_data.assign (Char_Count= chars)\n",
    "\n",
    "hashs= my_data[\"Body\"].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))   \n",
    "my_data= my_data.assign (Hashtags= hashs) #no. of hashtags in the body\n",
    "\n",
    "_timestamp = my_data[\"Created_at\"].apply(get_date)\n",
    "my_data = my_data.assign(timestamp = _timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Replace_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "Replace_bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_data(text):\n",
    "    text = text.lower()\n",
    "    text = Replace_by_space.sub(' ', text)\n",
    "    text = Replace_bad_symbols.sub(' ', text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)      # remove URLs\n",
    "    text = ' '.join(word for word in text.split() if word not in Stopwords)\n",
    "    return text\n",
    "\n",
    "def string_form(value):\n",
    "    return str(value)\n",
    "  \n",
    "def rare_words(text):\n",
    "    text= text.split()\n",
    "    freq = pd.Series(' '.join(my_data['Body']).split()).value_counts()[-10:]\n",
    "    freq = list(freq.index)\n",
    "    text = \" \".join(word for word in text if word not in freq)\n",
    "    #print (text)\n",
    "    return text\n",
    "\n",
    "my_data['Title'] = my_data['Title'].apply(string_form)\n",
    "my_data['Body'] = my_data['Body'].apply(string_form)\n",
    "my_data['Comments']=my_data['Comments'].apply(string_form)\n",
    "\n",
    "my_data['Title'] = my_data['Title'].apply(clean_data)\n",
    "my_data['Body'] = my_data['Body'].apply(clean_data)\n",
    "my_data['Comments']=my_data['Comments'].apply(clean_data)\n",
    "\n",
    "my_data['Body']= my_data['Body'].apply(rare_words)\n",
    "my_data['Comments']=my_data['Comments'].apply(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_data.to_csv('data_scraped6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
