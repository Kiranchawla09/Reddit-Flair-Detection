{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALLING MODULES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime as dt\n",
    "!pip install praw\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "import praw as pr                   #module for reddit\n",
    "from bs4 import BeautifulSoup       #Access the HTML of the webpage and extract useful information/data from it.\n",
    "                                    #It is of ‘string’ type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit = pr.Reddit(client_id='#', \n",
    "                     client_secret='#',\n",
    "                     user_agent='#',\n",
    "                     username='#', \n",
    "                     password='#')\n",
    "\n",
    "print(reddit.read_only)  \n",
    "reddit.read_only = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## India SubReddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subreddit = reddit.subreddit('india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#there are 5 organisations of the topics in the subreddit\n",
    "#that is, hot, new, controversial, top, rising\n",
    "flairs = [\"AskIndia\", \"Non-Political\", \"Policy/Economy\",\"AMA\", \"Politics\",\"[R]eddiquette\", \n",
    "          \"Photography\",\"Business/Finance\",\"Science/Technology\",\"Sports\", \"Scheduled\",\"Food\"]\n",
    "\n",
    "my_dict = { \"Flair\":[],\n",
    "             \"Title\":[],\n",
    "             \"Created_at\":[],\n",
    "             \"Score\":[],\n",
    "             \"Url_address\":[],\n",
    "             \"Comments\":[],\n",
    "             \"#Comments\":[],\n",
    "             \"Body\":[],\n",
    "             \"Author\":[],\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in flairs:\n",
    "    collect_subreddit= subreddit.search (f,limit=1000)\n",
    "    #collecting 1000 data for each flair \n",
    "    for i in collect_subreddit:\n",
    "        \n",
    "        my_dict[\"Flair\"].append (f)\n",
    "        my_dict[\"Title\"].append (i.title)\n",
    "        my_dict[\"Created_at\"].append (i.created)\n",
    "        my_dict[\"Score\"].append (i.score)\n",
    "        my_dict[\"Url_address\"].append(i.url)\n",
    "        my_dict[\"#Comments\"].append (i.num_comments)\n",
    "        my_dict[\"Body\"].append(i.selftext)\n",
    "        my_dict[\"Author\"].append (i.author)\n",
    "\n",
    "        i.comments.replace_more(limit=None)\n",
    "        comment = ''\n",
    "        for top_level_comment in i.comments:\n",
    "               comment = comment + ' ' + top_level_comment.body\n",
    "        my_dict[\"Comments\"].append(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.DataFrame(my_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(created):\n",
    "    return dt.datetime.fromtimestamp(created)\n",
    "\n",
    "def word_count (text):\n",
    "    return \n",
    "    \n",
    "words = my_data[\"Body\"].apply(lambda x: len(str(x).split(\" \")))   \n",
    "my_data= my_data.assign (Word_Count= words)      #no. of words in the body\n",
    "\n",
    "\n",
    "chars= my_data[\"Body\"].str.len()\n",
    "my_data= my_data.assign (Char_Count= chars)\n",
    "\n",
    "hashs= my_data[\"Body\"].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))   \n",
    "my_data= my_data.assign (Hashtags= hashs) #no. of hashtags in the body\n",
    "\n",
    "_timestamp = my_data[\"Created_at\"].apply(get_date)\n",
    "my_data = my_data.assign(timestamp = _timestamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "Replace_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "Replace_bad_symbols = re.compile('[^0-9a-z #+_]')\n",
    "Stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def clean_data(text):\n",
    "    text = text.lower()\n",
    "    text = Replace_by_space.sub(' ', text)\n",
    "    text = Replace_bad_symbols.sub(' ', text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', 'URL', text)      # remove URLs\n",
    "    text = ' '.join(word for word in text.split() if word not in Stopwords)\n",
    "    return text\n",
    "\n",
    "def string_form(value):\n",
    "    return str(value)\n",
    "  \n",
    "def rare_words(text):\n",
    "    text= text.split()\n",
    "    freq = pd.Series(' '.join(my_data['Body']).split()).value_counts()[-10:]\n",
    "    freq = list(freq.index)\n",
    "    text = \" \".join(word for word in text if word not in freq)\n",
    "    #print (text)\n",
    "    return text\n",
    "\n",
    "my_data['Title'] = my_data['Title'].apply(string_form)\n",
    "my_data['Body'] = my_data['Body'].apply(string_form)\n",
    "my_data['Comments']=my_data['Comments'].apply(string_form)\n",
    "\n",
    "my_data['Title'] = my_data['Title'].apply(clean_data)\n",
    "my_data['Body'] = my_data['Body'].apply(clean_data)\n",
    "my_data['Comments']=my_data['Comments'].apply(clean_data)\n",
    "\n",
    "my_data['Body']= my_data['Body'].apply(rare_words)\n",
    "my_data['Comments']=my_data['Comments'].apply(rare_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "my_data.to_csv('data_scraped6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
