{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Reddit Flair Detector.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kiranchawla09/Reddit-Flair-Detection/blob/master/Reddit_Flair_Detector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svbgAg_Qi8A2",
        "colab_type": "text"
      },
      "source": [
        "## Installing the Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCj0nhvGhrE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "5d0127f3-e7b5-43b7-d1b5-db8c515e2357"
      },
      "source": [
        "import pandas as pd\n",
        "import datetime as dt\n",
        "!pip install praw\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import re\n",
        "import praw as pr                   #module for reddit\n",
        "from bs4 import BeautifulSoup       #Access the HTML of the webpage and extract useful information/data from it.\n",
        "                                    #It is of ‘string’ type."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/87/95/1abd708fce27ba87ca9f17c6e68fe9d287666ad067432a7cd54d46424276/praw-6.3.1-py2.py3-none-any.whl (126kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 20kB 23.6MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 30kB 28.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 40kB 32.1MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 51kB 33.7MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 61kB 37.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 71kB 38.9MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 81kB 40.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 92kB 42.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 102kB 43.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 112kB 43.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 122kB 43.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 43.7MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.16 (from praw)\n",
            "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0 (from praw)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/19/44753eab1fdb50770ac69605527e8859468f3c0fd7dc5a76dd9c4dbd7906/websocket_client-0.56.0-py2.py3-none-any.whl (200kB)\n",
            "\r\u001b[K     |█▋                              | 10kB 24.5MB/s eta 0:00:01\r\u001b[K     |███▎                            | 20kB 29.1MB/s eta 0:00:01\r\u001b[K     |█████                           | 30kB 34.2MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 40kB 37.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 51kB 39.3MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 61kB 42.4MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 71kB 42.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 81kB 43.3MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 92kB 45.0MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 102kB 46.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 112kB 46.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 122kB 46.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 133kB 46.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 143kB 46.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 153kB 46.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 163kB 46.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 174kB 46.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 184kB 46.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 194kB 46.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 204kB 46.4MB/s \n",
            "\u001b[?25hCollecting prawcore<2.0,>=1.0.1 (from praw)\n",
            "  Downloading https://files.pythonhosted.org/packages/76/b5/ce6282dea45cba6f08a30e25d18e0f3d33277e2c9fcbda75644b8dc0089b/prawcore-1.0.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->praw) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2019.6.16)\n",
            "Installing collected packages: update-checker, websocket-client, prawcore, praw\n",
            "Successfully installed praw-6.3.1 prawcore-1.0.1 update-checker-0.16 websocket-client-0.56.0\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKn9Um3LjMlc",
        "colab_type": "text"
      },
      "source": [
        "## Loading the Application details"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLc2ikmlihZ0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "025349d5-cfe3-4d57-b6fe-f93874b03018"
      },
      "source": [
        "reddit = pr.Reddit(client_id='EuOy7dnVRlVDew', \n",
        "                     client_secret='SGDTw27oQfUc3i7S61rJhPknpgA',\n",
        "                     user_agent='reddit flair',\n",
        "                     username='kiran5_chawla', \n",
        "                     password='/Haathikhai')\n",
        "\n",
        "print(reddit.read_only)  \n",
        "reddit.read_only = True"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjIu8xivjmol",
        "colab_type": "text"
      },
      "source": [
        "#### India Reddit Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br8H0axbi5bx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "subreddit = reddit.subreddit('india')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKg8koPHNRt5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flairs = [\"AskIndia\", \"Non-Political\", \"Policy/Economy\",\"AMA\", \"Politics\",\"[R]eddiquette\", \n",
        "          \"Photography\",\"Business/Finance\",\"Science/Technology\",\"Sports\", \"Scheduled\",\"Food\"]\n",
        "\n",
        "my_dict = { \"Flair\":[],\n",
        "             \"Title\":[],\n",
        "             \"Created_at\":[],\n",
        "             \"Score\":[],\n",
        "             \"Url_address\":[],\n",
        "             \"Comments\":[],\n",
        "             \"#Comments\":[],\n",
        "             \"Body\":[],\n",
        "             \"Author\":[]\n",
        "           }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2MKEUCxjvgP",
        "colab_type": "text"
      },
      "source": [
        "## Collecting the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5uPE7JzNR7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for f in flairs:\n",
        "    collect_subreddit= subreddit.search (f,limit=1000)\n",
        "    #collecting 1000 data for each flair \n",
        "    for i in collect_subreddit:\n",
        "        \n",
        "        my_dict[\"Flair\"].append (f)\n",
        "        my_dict[\"Title\"].append (i.title)\n",
        "        my_dict[\"Created_at\"].append (i.created)\n",
        "        my_dict[\"Score\"].append (i.score)\n",
        "        my_dict[\"Url_address\"].append(i.url)\n",
        "        my_dict[\"#Comments\"].append (i.num_comments)\n",
        "        my_dict[\"Body\"].append(i.selftext)\n",
        "        my_dict[\"Author\"].append (i.author)\n",
        "\n",
        "        i.comments.replace_more(limit=None)\n",
        "        comment = ''\n",
        "        for top_level_comment in i.comments:\n",
        "               comment = comment + ' ' + top_level_comment.body\n",
        "        my_dict[\"Comments\"].append(comment)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XkQkDI2NR9i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_data = pd.DataFrame(my_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxY3upy8NR_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#my_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n26Q4NgeSJ7P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_date(created):\n",
        "    return dt.datetime.fromtimestamp(created)\n",
        "\n",
        "def word_count (text):\n",
        "    return \n",
        "    \n",
        "words = my_data[\"Body\"].apply(lambda x: len(str(x).split(\" \")))   \n",
        "my_data= my_data.assign (Word_Count= words)      #no. of words in the body\n",
        "\n",
        "\n",
        "chars= my_data[\"Body\"].str.len()\n",
        "my_data= my_data.assign (Char_Count= chars)\n",
        "\n",
        "hashs= my_data[\"Body\"].apply(lambda x: len([x for x in x.split() if x.startswith('#')]))   \n",
        "my_data= my_data.assign (Hashtags= hashs) #no. of hashtags in the body\n",
        "\n",
        "_timestamp = my_data[\"Created_at\"].apply(get_date)\n",
        "my_data = my_data.assign(timestamp = _timestamp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VLidEACQj7FM",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSIoG3E0SKRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "Replace_by_space = re.compile('[/(){}\\[\\]\\|@,;]')\n",
        "Replace_bad_symbols = re.compile('[^0-9a-z #+_]')\n",
        "Stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def clean_data(text):\n",
        "    text = text.lower()\n",
        "    text = Replace_by_space.sub(' ', text)\n",
        "    text = Replace_bad_symbols.sub(' ', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in Stopwords)\n",
        "    return text\n",
        "\n",
        "def string_form(value):\n",
        "    return str(value)\n",
        "   \n",
        "def rare_words(text):\n",
        "    text= text.split()\n",
        "    freq = pd.Series(' '.join(my_data['Body']).split()).value_counts()[-10:]\n",
        "    freq = list(freq.index)\n",
        "    text = \" \".join(word for word in text if word not in freq)\n",
        "    #print (text)\n",
        "    return text\n",
        "\n",
        "my_data['Title'] = my_data['Title'].apply(string_form)\n",
        "my_data['Body'] = my_data['Body'].apply(string_form)\n",
        "my_data['Comments']=my_data['Comments'].apply(string_form)\n",
        "\n",
        "my_data['Title'] = my_data['Title'].apply(clean_data)\n",
        "my_data['Body'] = my_data['Body'].apply(clean_data)\n",
        "my_data['Comments']=my_data['Comments'].apply(clean_data)\n",
        "\n",
        "my_data['Body']= my_data['Body'].apply(rare_words)\n",
        "my_data['Comments']=my_data['Comments'].apply(rare_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXnRpR8cj_Ri",
        "colab_type": "text"
      },
      "source": [
        "#### Saving to csv\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdtnP6t2Tlmq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_data.to_csv('data_scraped4.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFEZgD7XU4ot",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing the dataset\n",
        "dataset = pd.read_csv('data_scraped4.csv')\n",
        "X= dataset.iloc [:, [1,2,3,4,5,6,7,8,9,10,11,12]]\n",
        "y= dataset.iloc [:,0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1nfWdrTg-dt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Splitting the dataset into the Training set and Test set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.30, random_state = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}